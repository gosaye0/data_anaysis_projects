{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a850e2a3-a857-4e89-be49-3e315acc8b91",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef3c51c5-f47c-4ef7-b41f-99dac42a6607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Cleaning and Preprocessing\n",
      "==================================================\n",
      "Analysis Date: 2025-09-01 14:37\n"
     ]
    }
   ],
   "source": [
    "# Import neccessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category = FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category = UserWarning)\n",
    "\n",
    "# Set display options for pandas series and dataframs\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", None)\n",
    "\n",
    "print(\"Data Cleaning and Preprocessing\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215042e9-b39e-4476-8c63-b7ec95f9b8c3",
   "metadata": {},
   "source": [
    "### _1. Load and Document All Datasets_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1668354-6603-4f7f-9c53-e244ce3a025e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All datasets loades successfully\n"
     ]
    }
   ],
   "source": [
    "# file paths\n",
    "rwa_data_path = '../data/raw/'\n",
    "processed_data_path = '../data/processed/'\n",
    "\n",
    "# Load all neccessary datasets with error handling\n",
    "try:\n",
    "    orders = pd.read_csv(f'{rwa_data_path}olist_orders_dataset.csv')\n",
    "    order_items = pd.read_csv(f'{rwa_data_path}olist_order_items_dataset.csv')\n",
    "    customers = pd.read_csv(f'{rwa_data_path}olist_customers_dataset.csv')\n",
    "    products = pd.read_csv(f'{rwa_data_path}olist_products_dataset.csv')\n",
    "    sellers = pd.read_csv(f'{rwa_data_path}olist_sellers_dataset.csv')\n",
    "    reviews = pd.read_csv(f'{rwa_data_path}olist_order_reviews_dataset.csv')\n",
    "    payments = pd.read_csv(f'{rwa_data_path}olist_order_payments_dataset.csv')\n",
    "    geolocation = pd.read_csv(f'{rwa_data_path}olist_geolocation_dataset.csv')\n",
    "\n",
    "    print(\"All datasets loades successfully\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b20d5d-e227-42e5-9890-9bdf338732d2",
   "metadata": {},
   "source": [
    "### _2. Preview the Datasets_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89c52df8-2003-4d3c-a66f-e6e7b0ce2a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA PREVIEW: ORDERS\n",
      "============================================================\n",
      "Column Names:\n",
      "['order_id', 'customer_id', 'order_status', 'order_purchase_timestamp', 'order_approved_at', 'order_delivered_carrier_date', 'order_delivered_customer_date', 'order_estimated_delivery_date']\n",
      "\n",
      "5 Sample Rows:\n",
      "                           order_id                       customer_id  \\\n",
      "0  e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
      "1  53cdb2fc8bc7dce0b6741e2150273451  b0830fb4747a6c6d20dea0b8c802d7ef   \n",
      "2  47770eb9100c2d0c44946d9cf07ec65d  41ce2a54c0b03bf3443c3d931a367089   \n",
      "3  949d5b44dbf5de918fe9c16f97b45f8a  f88197465ea7920adcdbec7375364d82   \n",
      "4  ad21c59c0840e6cb83a9ceb5573f8159  8ab97904e6daea8866dbdbc4fb7aad2c   \n",
      "\n",
      "  order_status order_purchase_timestamp    order_approved_at  \\\n",
      "0    delivered      2017-10-02 10:56:33  2017-10-02 11:07:15   \n",
      "1    delivered      2018-07-24 20:41:37  2018-07-26 03:24:27   \n",
      "2    delivered      2018-08-08 08:38:49  2018-08-08 08:55:23   \n",
      "3    delivered      2017-11-18 19:28:06  2017-11-18 19:45:59   \n",
      "4    delivered      2018-02-13 21:18:39  2018-02-13 22:20:29   \n",
      "\n",
      "  order_delivered_carrier_date order_delivered_customer_date  \\\n",
      "0          2017-10-04 19:55:00           2017-10-10 21:25:13   \n",
      "1          2018-07-26 14:31:00           2018-08-07 15:27:45   \n",
      "2          2018-08-08 13:50:00           2018-08-17 18:06:29   \n",
      "3          2017-11-22 13:39:59           2017-12-02 00:28:42   \n",
      "4          2018-02-14 19:46:34           2018-02-16 18:17:02   \n",
      "\n",
      "  order_estimated_delivery_date  \n",
      "0           2017-10-18 00:00:00  \n",
      "1           2018-08-13 00:00:00  \n",
      "2           2018-09-04 00:00:00  \n",
      "3           2017-12-15 00:00:00  \n",
      "4           2018-02-26 00:00:00  \n",
      "\n",
      "\n",
      "\n",
      "DATA PREVIEW: ORDER_ITEMS\n",
      "============================================================\n",
      "Column Names:\n",
      "['order_id', 'order_item_id', 'product_id', 'seller_id', 'shipping_limit_date', 'price', 'freight_value']\n",
      "\n",
      "5 Sample Rows:\n",
      "                           order_id  order_item_id  \\\n",
      "0  00010242fe8c5a6d1ba2dd792cb16214              1   \n",
      "1  00018f77f2f0320c557190d7a144bdd3              1   \n",
      "2  000229ec398224ef6ca0657da4fc703e              1   \n",
      "3  00024acbcdf0a6daa1e931b038114c75              1   \n",
      "4  00042b26cf59d7ce69dfabb4e55b4fd9              1   \n",
      "\n",
      "                         product_id                         seller_id  \\\n",
      "0  4244733e06e7ecb4970a6e2683c13e61  48436dade18ac8b2bce089ec2a041202   \n",
      "1  e5f2d52b802189ee658865ca93d83a8f  dd7ddc04e1b6c2c614352b383efe2d36   \n",
      "2  c777355d18b72b67abbeef9df44fd0fd  5b51032eddd242adc84c38acab88f23d   \n",
      "3  7634da152a4610f1595efa32f14722fc  9d7a1d34a5052409006425275ba1c2b4   \n",
      "4  ac6c3623068f30de03045865e4e10089  df560393f3a51e74553ab94004ba5c87   \n",
      "\n",
      "   shipping_limit_date   price  freight_value  \n",
      "0  2017-09-19 09:45:35   58.90          13.29  \n",
      "1  2017-05-03 11:05:13  239.90          19.93  \n",
      "2  2018-01-18 14:48:30  199.00          17.87  \n",
      "3  2018-08-15 10:10:18   12.99          12.79  \n",
      "4  2017-02-13 13:57:51  199.90          18.14  \n",
      "\n",
      "\n",
      "\n",
      "DATA PREVIEW: CUSTOMERS\n",
      "============================================================\n",
      "Column Names:\n",
      "['customer_id', 'customer_unique_id', 'customer_zip_code_prefix', 'customer_city', 'customer_state']\n",
      "\n",
      "5 Sample Rows:\n",
      "                        customer_id                customer_unique_id  \\\n",
      "0  06b8999e2fba1a1fbc88172c00ba8bc7  861eff4711a542e4b93843c6dd7febb0   \n",
      "1  18955e83d337fd6b2def6b18a428ac77  290c77bc529b7ac935b93aa66c333dc3   \n",
      "2  4e7b3e00288586ebd08712fdd0374a03  060e732b5b29e8181a18229c7b0b2b5e   \n",
      "3  b2b6027bc5c5109e529d4dc6358b12c3  259dac757896d24d7702b9acbbff3f3c   \n",
      "4  4f2d8ab171c80ec8364f7c12e35b23ad  345ecd01c38d18a9036ed96c73b8d066   \n",
      "\n",
      "   customer_zip_code_prefix          customer_city customer_state  \n",
      "0                     14409                 franca             SP  \n",
      "1                      9790  sao bernardo do campo             SP  \n",
      "2                      1151              sao paulo             SP  \n",
      "3                      8775        mogi das cruzes             SP  \n",
      "4                     13056               campinas             SP  \n",
      "\n",
      "\n",
      "\n",
      "DATA PREVIEW: PRODUCTS\n",
      "============================================================\n",
      "Column Names:\n",
      "['product_id', 'product_category_name', 'product_name_lenght', 'product_description_lenght', 'product_photos_qty', 'product_weight_g', 'product_length_cm', 'product_height_cm', 'product_width_cm']\n",
      "\n",
      "5 Sample Rows:\n",
      "                         product_id  product_category_name  \\\n",
      "0  1e9e8ef04dbcff4541ed26657ea517e5             perfumaria   \n",
      "1  3aa071139cb16b67ca9e5dea641aaa2f                  artes   \n",
      "2  96bd76ec8810374ed1b65e291975717f          esporte_lazer   \n",
      "3  cef67bcfe19066a932b7673e239eb23d                  bebes   \n",
      "4  9dc1a7de274444849c219cff195d0b71  utilidades_domesticas   \n",
      "\n",
      "   product_name_lenght  product_description_lenght  product_photos_qty  \\\n",
      "0                 40.0                       287.0                 1.0   \n",
      "1                 44.0                       276.0                 1.0   \n",
      "2                 46.0                       250.0                 1.0   \n",
      "3                 27.0                       261.0                 1.0   \n",
      "4                 37.0                       402.0                 4.0   \n",
      "\n",
      "   product_weight_g  product_length_cm  product_height_cm  product_width_cm  \n",
      "0             225.0               16.0               10.0              14.0  \n",
      "1            1000.0               30.0               18.0              20.0  \n",
      "2             154.0               18.0                9.0              15.0  \n",
      "3             371.0               26.0                4.0              26.0  \n",
      "4             625.0               20.0               17.0              13.0  \n",
      "\n",
      "\n",
      "\n",
      "DATA PREVIEW: SELLERS\n",
      "============================================================\n",
      "Column Names:\n",
      "['seller_id', 'seller_zip_code_prefix', 'seller_city', 'seller_state']\n",
      "\n",
      "5 Sample Rows:\n",
      "                          seller_id  seller_zip_code_prefix  \\\n",
      "0  3442f8959a84dea7ee197c632cb2df15                   13023   \n",
      "1  d1b65fc7debc3361ea86b5f14c68d2e2                   13844   \n",
      "2  ce3ad9de960102d0677a81f5d0bb7b2d                   20031   \n",
      "3  c0f3eea2e14555b6faeea3dd58c1b1c3                    4195   \n",
      "4  51a04a8a6bdcb23deccc82b0b80742cf                   12914   \n",
      "\n",
      "         seller_city seller_state  \n",
      "0           campinas           SP  \n",
      "1         mogi guacu           SP  \n",
      "2     rio de janeiro           RJ  \n",
      "3          sao paulo           SP  \n",
      "4  braganca paulista           SP  \n",
      "\n",
      "\n",
      "\n",
      "DATA PREVIEW: REVIEWS\n",
      "============================================================\n",
      "Column Names:\n",
      "['review_id', 'order_id', 'review_score', 'review_comment_title', 'review_comment_message', 'review_creation_date', 'review_answer_timestamp']\n",
      "\n",
      "5 Sample Rows:\n",
      "                          review_id                          order_id  \\\n",
      "0  7bc2406110b926393aa56f80a40eba40  73fc7af87114b39712e6da79b0a377eb   \n",
      "1  80e641a11e56f04c1ad469d5645fdfde  a548910a1c6147796b98fdf73dbeba33   \n",
      "2  228ce5500dc1d8e020d8d1322874b6f0  f9e4b658b201a9f2ecdecbb34bed034b   \n",
      "3  e64fb393e7b32834bb789ff8bb30750e  658677c97b385a9be170737859d3511b   \n",
      "4  f7c4243c7fe1938f181bec41a392bdeb  8e6bfb81e283fa7e4f11123a3fb894f1   \n",
      "\n",
      "   review_score review_comment_title  \\\n",
      "0             4                  NaN   \n",
      "1             5                  NaN   \n",
      "2             5                  NaN   \n",
      "3             5                  NaN   \n",
      "4             5                  NaN   \n",
      "\n",
      "                              review_comment_message review_creation_date  \\\n",
      "0                                                NaN  2018-01-18 00:00:00   \n",
      "1                                                NaN  2018-03-10 00:00:00   \n",
      "2                                                NaN  2018-02-17 00:00:00   \n",
      "3              Recebi bem antes do prazo estipulado.  2017-04-21 00:00:00   \n",
      "4  ParabÃ©ns lojas lannister adorei comprar pela I...  2018-03-01 00:00:00   \n",
      "\n",
      "  review_answer_timestamp  \n",
      "0     2018-01-18 21:46:59  \n",
      "1     2018-03-11 03:05:13  \n",
      "2     2018-02-18 14:36:24  \n",
      "3     2017-04-21 22:02:06  \n",
      "4     2018-03-02 10:26:53  \n",
      "\n",
      "\n",
      "\n",
      "DATA PREVIEW: PAYMENTS\n",
      "============================================================\n",
      "Column Names:\n",
      "['order_id', 'payment_sequential', 'payment_type', 'payment_installments', 'payment_value']\n",
      "\n",
      "5 Sample Rows:\n",
      "                           order_id  payment_sequential payment_type  \\\n",
      "0  b81ef226f3fe1789b1e8b2acac839d17                   1  credit_card   \n",
      "1  a9810da82917af2d9aefd1278f1dcfa0                   1  credit_card   \n",
      "2  25e8ea4e93396b6fa0d3dd708e76c1bd                   1  credit_card   \n",
      "3  ba78997921bbcdc1373bb41e913ab953                   1  credit_card   \n",
      "4  42fdf880ba16b47b59251dd489d4441a                   1  credit_card   \n",
      "\n",
      "   payment_installments  payment_value  \n",
      "0                     8          99.33  \n",
      "1                     1          24.39  \n",
      "2                     1          65.71  \n",
      "3                     8         107.78  \n",
      "4                     2         128.45  \n",
      "\n",
      "\n",
      "\n",
      "DATA PREVIEW: GEOLOCATION\n",
      "============================================================\n",
      "Column Names:\n",
      "['geolocation_zip_code_prefix', 'geolocation_lat', 'geolocation_lng', 'geolocation_city', 'geolocation_state']\n",
      "\n",
      "5 Sample Rows:\n",
      "   geolocation_zip_code_prefix  geolocation_lat  geolocation_lng  \\\n",
      "0                         1037       -23.545621       -46.639292   \n",
      "1                         1046       -23.546081       -46.644820   \n",
      "2                         1046       -23.546129       -46.642951   \n",
      "3                         1041       -23.544392       -46.639499   \n",
      "4                         1035       -23.541578       -46.641607   \n",
      "\n",
      "  geolocation_city geolocation_state  \n",
      "0        sao paulo                SP  \n",
      "1        sao paulo                SP  \n",
      "2        sao paulo                SP  \n",
      "3        sao paulo                SP  \n",
      "4        sao paulo                SP  \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def dataset_preview(dataset, dataset_name):\n",
    "    \"\"\"\n",
    "    Data preview function\n",
    "    \n",
    "    Parameters:\n",
    "    dataset (DataFrame): Dataset to assess\n",
    "    dataset_name (str): Name of the dataset for reporting\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    print(f\"DATA PREVIEW: {dataset_name.upper()}\")\n",
    "    print('=' * 60)\n",
    "    \n",
    "    print(f\"Column Names:\\n{dataset.columns.to_list()}\")\n",
    "    print(f\"\\n5 Sample Rows:\\n{dataset.head()}\")\n",
    "\n",
    "datasets = {\n",
    "    'orders': orders,\n",
    "    'order_items': order_items,\n",
    "    'customers': customers,\n",
    "    'products': products,\n",
    "    'sellers': sellers,\n",
    "    'reviews': reviews,\n",
    "    'payments': payments,\n",
    "    'geolocation': geolocation\n",
    "}\n",
    "for name, df in datasets.items():\n",
    "    dataset_preview(df, name)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4298bd-30c4-4960-96fb-5b3ba263f785",
   "metadata": {},
   "source": [
    "### _3. Assess Data Quality_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdba3aef-976b-41ae-9b74-50ef25aa43f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA QUALITY REPORT: ORDERS\n",
      "============================================================\n",
      "Shape: 99441 rows x 8 columns\n",
      "Memory usage: 58.97 MB\n",
      "\n",
      " Missing Valuse Found: \n",
      "                               Missing Count  Missing Percentage\n",
      "order_delivered_customer_date           2965            2.981668\n",
      "order_delivered_carrier_date            1783            1.793023\n",
      "order_approved_at                        160            0.160899\n",
      "\n",
      " Data Types:\n",
      "object    8\n",
      "Name: count, dtype: int64\n",
      "Duplicated rows: 0\n",
      "\n",
      "\n",
      "\n",
      "DATA QUALITY REPORT: ORDER_ITEMS\n",
      "============================================================\n",
      "Shape: 112650 rows x 7 columns\n",
      "Memory usage: 39.43 MB\n",
      "No missing valuse found.\n",
      "\n",
      " Data Types:\n",
      "object     4\n",
      "float64    2\n",
      "int64      1\n",
      "Name: count, dtype: int64\n",
      "Duplicated rows: 0\n",
      "\n",
      "\n",
      "\n",
      "DATA QUALITY REPORT: CUSTOMERS\n",
      "============================================================\n",
      "Shape: 99441 rows x 5 columns\n",
      "Memory usage: 29.62 MB\n",
      "No missing valuse found.\n",
      "\n",
      " Data Types:\n",
      "object    4\n",
      "int64     1\n",
      "Name: count, dtype: int64\n",
      "Duplicated rows: 0\n",
      "\n",
      "\n",
      "\n",
      "DATA QUALITY REPORT: PRODUCTS\n",
      "============================================================\n",
      "Shape: 32951 rows x 9 columns\n",
      "Memory usage: 6.79 MB\n",
      "\n",
      " Missing Valuse Found: \n",
      "                            Missing Count  Missing Percentage\n",
      "product_category_name                 610            1.851234\n",
      "product_description_lenght            610            1.851234\n",
      "product_name_lenght                   610            1.851234\n",
      "product_photos_qty                    610            1.851234\n",
      "product_weight_g                        2            0.006070\n",
      "product_height_cm                       2            0.006070\n",
      "product_length_cm                       2            0.006070\n",
      "product_width_cm                        2            0.006070\n",
      "\n",
      " Data Types:\n",
      "float64    7\n",
      "object     2\n",
      "Name: count, dtype: int64\n",
      "Duplicated rows: 0\n",
      "\n",
      "\n",
      "\n",
      "DATA QUALITY REPORT: SELLERS\n",
      "============================================================\n",
      "Shape: 3095 rows x 4 columns\n",
      "Memory usage: 0.66 MB\n",
      "No missing valuse found.\n",
      "\n",
      " Data Types:\n",
      "object    3\n",
      "int64     1\n",
      "Name: count, dtype: int64\n",
      "Duplicated rows: 0\n",
      "\n",
      "\n",
      "\n",
      "DATA QUALITY REPORT: REVIEWS\n",
      "============================================================\n",
      "Shape: 99224 rows x 7 columns\n",
      "Memory usage: 42.75 MB\n",
      "\n",
      " Missing Valuse Found: \n",
      "                        Missing Count  Missing Percentage\n",
      "review_comment_title            87656           88.341530\n",
      "review_comment_message          58247           58.702532\n",
      "\n",
      " Data Types:\n",
      "object    6\n",
      "int64     1\n",
      "Name: count, dtype: int64\n",
      "Duplicated rows: 0\n",
      "\n",
      "\n",
      "\n",
      "DATA QUALITY REPORT: PAYMENTS\n",
      "============================================================\n",
      "Shape: 103886 rows x 5 columns\n",
      "Memory usage: 17.81 MB\n",
      "No missing valuse found.\n",
      "\n",
      " Data Types:\n",
      "object     2\n",
      "int64      2\n",
      "float64    1\n",
      "Name: count, dtype: int64\n",
      "Duplicated rows: 0\n",
      "\n",
      "\n",
      "\n",
      "DATA QUALITY REPORT: GEOLOCATION\n",
      "============================================================\n",
      "Shape: 1000163 rows x 5 columns\n",
      "Memory usage: 145.20 MB\n",
      "No missing valuse found.\n",
      "\n",
      " Data Types:\n",
      "float64    2\n",
      "object     2\n",
      "int64      1\n",
      "Name: count, dtype: int64\n",
      "Duplicated rows: 261831\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def assess_data_quality(dataset, dataset_name):\n",
    "    \"\"\"\n",
    "    Data quality assessment function\n",
    "    \n",
    "    Parameters:\n",
    "    dataset (DataFrame): Dataset to assess\n",
    "    dataset_name (str): Name of the dataset for reporting\n",
    "    \n",
    "    Returns:\n",
    "    dict: Summary of data quality metrics\n",
    "    \"\"\"\n",
    "    print(f\"DATA QUALITY REPORT: {dataset_name.upper()}\")\n",
    "    print('=' * 60)\n",
    "    \n",
    "    print(f\"Shape: {dataset.shape[0]} rows x {dataset.shape[1]} columns\")\n",
    "    print(f\"Memory usage: {dataset.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "    # Missing valuse analysis\n",
    "    missing_data = dataset.isna().sum()\n",
    "    missing_percent = (missing_data/len(dataset)) * 100\n",
    "    missing_data_df = pd.DataFrame(\n",
    "        {'Missing Count': missing_data,\n",
    "         'Missing Percentage': missing_percent}).sort_values('Missing Count', ascending = False)\n",
    "    if missing_data_df['Missing Count'].sum() > 0:\n",
    "        print(\"\\n Missing Valuse Found: \")\n",
    "        print(missing_data_df[missing_data_df['Missing Count'] > 0])\n",
    "    else:\n",
    "        print(\"No missing valuse found.\")\n",
    "\n",
    "    # Data types\n",
    "    print(\"\\n Data Types:\")\n",
    "    print(dataset.dtypes.value_counts())\n",
    "\n",
    "    # Duplicates\n",
    "    duplicates = dataset.duplicated().sum()\n",
    "    print(f\"Duplicated rows: {duplicates}\")\n",
    "\n",
    "    return {\n",
    "        'shape': dataset.shape,\n",
    "        'missing_valuse': missing_data_df[missing_data_df['Missing Count'] > 0],\n",
    "        'duplicates': duplicates,\n",
    "        'memory_mb': dataset.memory_usage(deep=True).sum() / 1024**2\n",
    "    }\n",
    "\n",
    "datasets = {\n",
    "    'orders': orders,\n",
    "    'order_items': order_items,\n",
    "    'customers': customers,\n",
    "    'products': products,\n",
    "    'sellers': sellers,\n",
    "    'reviews': reviews,\n",
    "    'payments': payments,\n",
    "    'geolocation': geolocation\n",
    "}\n",
    "data_quality = {}\n",
    "for name, df in datasets.items():\n",
    "    data_quality[name] = assess_data_quality(df, name)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2895c512-a615-4126-815b-c6872322516a",
   "metadata": {},
   "source": [
    "### _4. Data Type Conversion and optimization_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e3bb902-12f1-4b13-905b-c7fa9b39122a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”§ DOWNCAST INT DATA TYPES: ORDERS\n",
      "==================================================\n",
      "Original memory usage: 58.97 MB\n",
      "New memory usage: 58.97 MB\n",
      "Memory reduction: 0.0%\n",
      "\n",
      "ðŸ”§ DOWNCAST INT DATA TYPES: ORDER_ITEMS\n",
      "==================================================\n",
      "Original memory usage: 39.43 MB\n",
      "New memory usage: 38.68 MB\n",
      "Memory reduction: 1.9%\n",
      "\n",
      "ðŸ”§ DOWNCAST INT DATA TYPES: CUSTOMERS\n",
      "==================================================\n",
      "Original memory usage: 29.62 MB\n",
      "New memory usage: 29.24 MB\n",
      "Memory reduction: 1.3%\n",
      "\n",
      "ðŸ”§ DOWNCAST INT DATA TYPES: PRODUCTS\n",
      "==================================================\n",
      "Original memory usage: 6.79 MB\n",
      "New memory usage: 6.79 MB\n",
      "Memory reduction: 0.0%\n",
      "\n",
      "ðŸ”§ DOWNCAST INT DATA TYPES: SELLERS\n",
      "==================================================\n",
      "Original memory usage: 0.66 MB\n",
      "New memory usage: 0.65 MB\n",
      "Memory reduction: 1.8%\n",
      "\n",
      "ðŸ”§ DOWNCAST INT DATA TYPES: REVIEWS\n",
      "==================================================\n",
      "Original memory usage: 42.75 MB\n",
      "New memory usage: 42.08 MB\n",
      "Memory reduction: 1.5%\n",
      "\n",
      "ðŸ”§ DOWNCAST INT DATA TYPES: PAYMENTS\n",
      "==================================================\n",
      "Original memory usage: 17.81 MB\n",
      "New memory usage: 16.43 MB\n",
      "Memory reduction: 7.8%\n",
      "\n",
      "ðŸ”§ DOWNCAST INT DATA TYPES: GEOLOCATION\n",
      "==================================================\n",
      "Original memory usage: 146.09 MB\n",
      "New memory usage: 142.27 MB\n",
      "Memory reduction: 2.6%\n"
     ]
    }
   ],
   "source": [
    "def downcast_int_columns(df, dataset_name):\n",
    "    \"\"\"\n",
    "    Downcast integer columns in a DataFrame to smaller integer types to save memory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The DataFrame containing integer columns to optimize.\n",
    "    dataset_name : str\n",
    "        Name of the dataset (used for display/logging purposes).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A copy of the DataFrame with integer columns downcasted.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    - Only int32 and int64 columns are downcasted.\n",
    "    - Prints memory usage before and after optimization.\n",
    "    \"\"\"\n",
    "    print(f\"\\nðŸ”§ DOWNCAST INT DATA TYPES: {dataset_name.upper()}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    original_memory = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f\"Original memory usage: {original_memory:.2f} MB\")\n",
    "    \n",
    "    # Only apply downcast to integer columns\n",
    "    int_cols = df.select_dtypes(include=['int64', 'int32']).columns\n",
    "    df[int_cols] = df[int_cols].apply(pd.to_numeric, downcast=\"integer\")\n",
    "    \n",
    "    new_memory = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f\"New memory usage: {new_memory:.2f} MB\")\n",
    "    print(f\"Memory reduction: {((original_memory - new_memory) / original_memory) * 100:.1f}%\")\n",
    "\n",
    "    return df\n",
    "\n",
    "datasets = {\n",
    "    'orders': orders,\n",
    "    'order_items': order_items,\n",
    "    'customers': customers,\n",
    "    'products': products,\n",
    "    'sellers': sellers,\n",
    "    'reviews': reviews,\n",
    "    'payments': payments,\n",
    "    'geolocation': geolocation\n",
    "}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    datasets[name] = downcast_int_columns(df, name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99032017-2946-467d-be87-c263f65a3cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_datetime(dataset, columns, format=None, errors='coerce'):\n",
    "    \"\"\"\n",
    "    Convert specified columns of a DataFrame to datetime.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : pd.DataFrame\n",
    "        DataFrame containing the columns to convert.\n",
    "    columns : list of str\n",
    "        Column names to convert to datetime.\n",
    "    format : str, optional\n",
    "        Datetime format (default: None, auto-infer).\n",
    "    errors : str, optional\n",
    "        How to handle errors: 'raise', 'coerce', or 'ignore' (default: 'coerce').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A copy of the DataFrame with the specified columns converted to datetime.\n",
    "    \"\"\"\n",
    "    dataset = dataset.copy()\n",
    "    for col in columns:\n",
    "        dataset[col] = pd.to_datetime(dataset[col], format=format, errors=errors)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e922a2ca-8443-497f-8b6a-6ec6076335eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_dateime_cols = ['order_purchase_timestamp', 'order_approved_at', 'order_delivered_carrier_date',\n",
    "                       'order_delivered_customer_date', 'order_estimated_delivery_date']\n",
    "\n",
    "order_items_datetime_cols = ['shipping_limit_date']\n",
    "reviews_datetime_cols = ['review_creation_date', 'review_answer_timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8b69551-3e62-4030-af78-ddfb393aa93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "order_purchase_timestamp         object\n",
      "order_approved_at                object\n",
      "order_delivered_carrier_date     object\n",
      "order_delivered_customer_date    object\n",
      "order_estimated_delivery_date    object\n",
      "dtype: object\n",
      "\n",
      "After:\n",
      "order_purchase_timestamp         datetime64[ns]\n",
      "order_approved_at                datetime64[ns]\n",
      "order_delivered_carrier_date     datetime64[ns]\n",
      "order_delivered_customer_date    datetime64[ns]\n",
      "order_estimated_delivery_date    datetime64[ns]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Orders date time data type conversion\n",
    "print(f\"Before:\\n{orders[orders_dateime_cols].dtypes}\")\n",
    "orders_clean = convert_to_datetime(orders, orders_dateime_cols)\n",
    "print(f\"\\nAfter:\\n{orders_clean[orders_dateime_cols].dtypes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf5b6f4d-5402-4281-8459-a801ed4692d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "shipping_limit_date    object\n",
      "dtype: object\n",
      "\n",
      "After:\n",
      "shipping_limit_date    datetime64[ns]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Order_items date time data type conversion\n",
    "print(f\"Before:\\n{order_items[order_items_datetime_cols].dtypes}\")\n",
    "order_items_clean = convert_to_datetime(order_items, order_items_datetime_cols)\n",
    "print(f\"\\nAfter:\\n{orders_items_clean[order_items_datetime_cols].dtypes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "198157fd-1e7a-46de-bb30-c633f109599f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "review_creation_date       object\n",
      "review_answer_timestamp    object\n",
      "dtype: object\n",
      "\n",
      "After:\n",
      "review_creation_date       datetime64[ns]\n",
      "review_answer_timestamp    datetime64[ns]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# reviews date time data type conversion\n",
    "print(f\"Before:\\n{reviews[reviews_datetime_cols].dtypes}\")\n",
    "reviews_clean = convert_to_datetime(reviews, reviews_datetime_cols)\n",
    "print(f\"\\nAfter:\\n{reviews_clean[reviews_datetime_cols].dtypes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec948dd-0f53-4684-b1b1-d884943e9ef8",
   "metadata": {},
   "source": [
    "### _5. Handling Missing Values_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b3aa5b-c9f0-42b1-acc3-232bfde1a213",
   "metadata": {},
   "source": [
    "We only have missing datas from two tables (orders and reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a09e7215-ebde-464b-8ba2-cab2f2831af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 4908 NaNs\n",
      "After: 0 NaNs\n"
     ]
    }
   ],
   "source": [
    "# Clean Orders dataset\n",
    "print(f\"Before: {orders_clean.isna().sum().sum()} NaNs\")\n",
    "orders_clean = orders_clean.dropna(subset = ['order_approved_at'])\n",
    "orders_clean = orders_clean.fillna({'order_delivered_customer_date': 'unkown', 'order_delivered_carrier_date': 'unknown'})\n",
    "print(f\"After: {orders_clean.isna().sum().sum()} NaNs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3fcc5a1-3870-453d-9003-d930fcb7e73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 145903 NaNs\n",
      "After: 0 NaNs\n"
     ]
    }
   ],
   "source": [
    "# Clean Reviews dataset\n",
    "print(f\"Before: {reviews_clean.isna().sum().sum()} NaNs\")\n",
    "reviews_clean  = reviews_clean.fillna({'review_comment_title': 'No Title', 'review_comment_message': 'No Message'})\n",
    "print(f\"After: {reviews_clean.isna().sum().sum()} NaNs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01a3525-885b-47ed-ad5d-b729542e49c0",
   "metadata": {},
   "source": [
    "### _6. Creating Master Dataset_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bbe70910-3de9-4604-9dec-218e277c6273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CREATING MASTER DATASET\n",
      "==============================\n",
      "Starting with orders: 99281 records\n",
      "After adding customers: 99281 records\n",
      "After adding order items: 99281 records\n",
      "After adding reviews: 99281 records\n",
      "Final master dataset: 99281 records\n"
     ]
    }
   ],
   "source": [
    "def create_master_dataset(orders_df, items_df, customers_df, reviews_df, payments_df):\n",
    "    \"\"\"\n",
    "    Create a master dataset by intelligently joining related tables\n",
    "    \"\"\"\n",
    "    print(\"\\nCREATING MASTER DATASET\")\n",
    "    print(\"=\" * 30)\n",
    "\n",
    "    # Start with orders as the base\n",
    "    master = orders_df.copy()\n",
    "    print(f\"Starting with orders: {len(master)} records\")\n",
    "\n",
    "    # Adding customer information\n",
    "    master = master.merge(customers_df, on = 'customer_id', how = 'left')\n",
    "    print(f\"After adding customers: {len(master)} records\")\n",
    "\n",
    "    # Add order items\n",
    "    items_agg = order_items.groupby('order_id').agg({\n",
    "        'price': ['sum', 'count', 'mean'],\n",
    "        'freight_value': 'sum',\n",
    "        'product_id': 'nunique'\n",
    "    }).round(2)\n",
    "\n",
    "    # Flatten column names\n",
    "    items_agg.columns = ['_'.join(col).strip() for col in items_agg.columns.values]\n",
    "    items_agg = items_agg.reset_index()\n",
    "\n",
    "    master = master.merge(items_agg, on = 'order_id', how = 'left')\n",
    "    print(f\"After adding order items: {len(master)} records\")\n",
    "\n",
    "    # Add reviews scores\n",
    "    reviews_agg = reviews_df.groupby('order_id').agg({\n",
    "        'review_score': 'mean'\n",
    "    }).round(2).reset_index()\n",
    "    \n",
    "    master = master.merge(reviews_agg, on='order_id', how='left')\n",
    "    print(f\"After adding reviews: {len(master)} records\")\n",
    "    \n",
    "    # Add payment information\n",
    "    payments_agg = payments_df.groupby('order_id').agg({\n",
    "        'payment_value': 'sum',\n",
    "        'payment_installments': 'max'\n",
    "    }).reset_index()\n",
    "    \n",
    "    master = master.merge(payments_agg, on='order_id', how='left')\n",
    "    print(f\"Final master dataset: {len(master)} records\")\n",
    "    \n",
    "    return master\n",
    "\n",
    "# Create the master dataset\n",
    "master_dataset = create_master_dataset(\n",
    "    orders_clean, order_items_clean, customers, \n",
    "    reviews_clean, payments.copy()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1243f914-87e4-412b-854d-4ef4bb45989d",
   "metadata": {},
   "source": [
    "### _7. Save Processed data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "855a5184-a34b-4c1a-b3b7-ce40e66a23a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Processed Data\n",
      "============================================================\n",
      "Saved orders_clean: 99,281 rows Ã— 8 columns\n",
      "Saved customers_clean: 99,441 rows Ã— 5 columns\n",
      "Saved order_items_clean: 112,650 rows Ã— 7 columns\n",
      "Saved reviews_clean: 99,224 rows Ã— 7 columns\n",
      "Saved master_dataset: 99,281 rows Ã— 20 columns\n",
      "\n",
      " All processed data saved to: ../data/processed/\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving Processed Data\")\n",
    "print('=' * 60)\n",
    "\n",
    "datasets_to_save = {\n",
    "    'orders_clean': orders_clean,\n",
    "    'customers_clean': customers,\n",
    "    'order_items_clean': order_items_clean,\n",
    "    'reviews_clean': reviews_clean,\n",
    "    'master_dataset': master_dataset\n",
    "}\n",
    "\n",
    "for name, df in datasets_to_save.items():\n",
    "    filepath = f'{processed_data_path}{name}.csv'\n",
    "    df.to_csv(filepath, index=False)\n",
    "    print(f\"Saved {name}: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n",
    "\n",
    "print(f\"\\n All processed data saved to: {processed_data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677840f9-5f4e-436c-9018-5cce90e4d26d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (data-analysis)",
   "language": "python",
   "name": "data-anaysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
